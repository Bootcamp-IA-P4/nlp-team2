# Sistema de Tests - NLP Team 2

Este README describe exclusivamente el sistema de testing del proyecto: cÃ³mo funciona, cÃ³mo ejecutarlo y cÃ³mo interpretar los resultados.
Este conjunto de tests esta desarrollado con Copilot integramente utilizando 'IngenierÃ­a de Pronts'.

## ğŸ“‹ Tabla de Contenidos

- [Estructura de Tests](#estructura-de-tests)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [EjecuciÃ³n de Tests](#ejecuciÃ³n-de-tests)
- [Cobertura de CÃ³digo](#cobertura-de-cÃ³digo)
- [Tipos de Tests](#tipos-de-tests)
- [InterpretaciÃ³n de Resultados](#interpretaciÃ³n-de-resultados)
- [SoluciÃ³n de Problemas](#soluciÃ³n-de-problemas)

## ğŸ§ª Estructura de Tests

```
server/tests/
â”œâ”€â”€ README.md             # Este archivo
â”œâ”€â”€ conftest.py           # ConfiguraciÃ³n y fixtures de pytest
â”œâ”€â”€ pytest.ini            # ConfiguraciÃ³n de pytest
â”œâ”€â”€ .coveragerc           # ConfiguraciÃ³n de cobertura
â”œâ”€â”€ run_coverage.sh       # Script para ejecutar tests con cobertura
â”œâ”€â”€ test_print_dev.py     # Tests del mÃ³dulo de logging (24 tests)
â”œâ”€â”€ test_scrp.py          # Tests del scraper (23 tests)
â”œâ”€â”€ test_database.py      # Tests del gestor de base de datos (18 tests)
â””â”€â”€ test_main.py          # Tests unificados del mÃ³dulo principal (29 tests)
```

**Total de Tests**: 94 tests unitarios y de integraciÃ³n

## âš™ï¸ ConfiguraciÃ³n

### Dependencias
El sistema de tests utiliza:
- `pytest` - Framework principal de testing
- `pytest-cov` - Plugin para mediciÃ³n de cobertura
- `coverage` - Herramienta de anÃ¡lisis de cobertura
- `unittest.mock` - Para mocking y simulaciÃ³n

### Archivos de ConfiguraciÃ³n

#### `pytest.ini`
ConfiguraciÃ³n principal de pytest con opciones de ejecuciÃ³n y reportes.

#### `.coveragerc`
ConfiguraciÃ³n de cobertura que especifica:
- Archivos a incluir/excluir
- Directorios a analizar
- Formato de reportes

#### `conftest.py`
Contiene fixtures reutilizables y configuraciÃ³n global:
- Fixtures de base de datos mock
- Fixtures para logging
- Fixtures para scraper
- ConfiguraciÃ³n de paths del proyecto

## ğŸš€ EjecuciÃ³n de Tests

### MÃ©todo 1: Script de Cobertura (Recomendado)
```bash
# Desde la carpeta tests/
./run_coverage.sh
```

Este script ejecuta todos los tests y genera reportes de cobertura completos.

### MÃ©todo 2: Comandos Directos

#### Ejecutar todos los tests
```bash
cd server/tests
python -m pytest -v
```

#### Ejecutar tests con cobertura
```bash
cd server/tests
python -m pytest --cov=../ --cov-report=html --cov-report=term -v
```

#### Ejecutar tests especÃ­ficos
```bash
# Test de un mÃ³dulo especÃ­fico
python -m pytest test_print_dev.py -v

# Test de una funciÃ³n especÃ­fica
python -m pytest test_main.py::test_create_app -v

# Tests por patrÃ³n
python -m pytest -k "database" -v
```

#### Ejecutar con diferentes niveles de verbosidad
```bash
# BÃ¡sico
python -m pytest

# Verboso
python -m pytest -v

# Extra verboso
python -m pytest -vv

# Con salida en tiempo real
python -m pytest -s
```

## ğŸ“Š Cobertura de CÃ³digo

### Generar Reporte de Cobertura
```bash
# Reporte en terminal y HTML
python -m pytest --cov=../ --cov-report=html --cov-report=term

# Solo reporte HTML
python -m pytest --cov=../ --cov-report=html

# Solo reporte en terminal
python -m pytest --cov=../ --cov-report=term
```

### Ver Reportes
- **Terminal**: Se muestra automÃ¡ticamente tras la ejecuciÃ³n
- **HTML**: Abre `htmlcov/index.html` en tu navegador
- **Archivos especÃ­ficos**: `htmlcov/[nombre_archivo].html`

### Estado Actual de Cobertura
- **core/config.py**: 100% (7 lÃ­neas)
- **core/print_dev.py**: 96% (57 lÃ­neas, 2 sin cubrir)
- **database/models.py**: 100% (41 lÃ­neas)
- **scraper/scrp.py**: 24% (359 lÃ­neas, 273 sin cubrir)
- **database/db_manager.py**: 32% (122 lÃ­neas, 83 sin cubrir)
- **main.py**: 11% (19 lÃ­neas, 17 sin cubrir)
- **test_print_dev.py**: 99% (147 lÃ­neas, 2 sin cubrir)
- **test_main.py**: 89% (474 lÃ­neas, 53 sin cubrir)
- **test_scrp.py**: 100% (195 lÃ­neas)
- **test_database.py**: 100% (200 lÃ­neas)
- **Total del proyecto**: 59% (2092 lÃ­neas, 851 sin cubrir)

## ğŸ”¬ Tipos de Tests

### Tests Unitarios
- **test_print_dev.py**: Funciones de logging y impresiÃ³n (24 tests)
- **test_scrp.py**: Funciones del scraper de YouTube (23 tests)
- **test_database.py**: Operaciones de base de datos (18 tests)

### Tests de IntegraciÃ³n
- **test_main.py**: Tests unificados del mÃ³dulo principal (29 tests)
  - Tests de estructura y configuraciÃ³n
  - Tests de importaciÃ³n y dependencias
  - Tests de cobertura forzada
  - Tests de ejecuciÃ³n real con mocks
  - Tests avanzados para maximizar cobertura

### Tipos de Mocking
- **Mock de Base de Datos**: Simula conexiones SQLAlchemy
- **Mock de Selenium**: Simula navegador web para scraping
- **Mock de APIs**: Simula respuestas de servicios externos
- **Mock de Sistema**: Simula operaciones del sistema operativo

## ğŸ“ˆ InterpretaciÃ³n de Resultados

### Salida de Tests Exitosos
```
==================== test session starts ====================
collected 94 items

test_print_dev.py ........................               [ 25%]
test_scrp.py .............................               [ 50%]
test_database.py .........................               [ 69%]
test_main.py .............................               [100%]

==================== 94 passed in 12.73s ====================
```

### Salida de Cobertura
```
Name                      Stmts   Miss  Cover
-------------------------------------------------------
core/config.py                7      0   100%
core/print_dev.py            57      2    96%
database/models.py           41      0   100%
scraper/scrp.py             359    273    24%
database/db_manager.py      122     83    32%
main.py                      19     17    11%
test_print_dev.py           147      2    99%
test_main.py                474     53    89%
test_scrp.py                195      0   100%
test_database.py            200      0   100%
-------------------------------------------------------
TOTAL                      2092    851    59%
```

### SÃ­mbolos de Estado
- âœ… `.` = Test pasÃ³
- âŒ `F` = Test fallÃ³
- âš ï¸ `E` = Error en el test
- â­ï¸ `s` = Test saltado
- â“ `x` = Fallo esperado

### Resumen de Estado Actual
- **Total de Tests**: 94 tests unitarios y de integraciÃ³n
- **Tests Exitosos**: 94/94 (100% de Ã©xito)
- **Tests Fallidos**: 0
- **Warnings**: 2 (deprecation de asyncio en Python 3.14)

## ğŸ› ï¸ SoluciÃ³n de Problemas

### Errores Comunes

#### "ModuleNotFoundError"
```bash
# Ejecutar desde la carpeta correcta
cd server/tests
python -m pytest
```

#### "No tests ran matching the given pattern"
```bash
# Verificar nombres de archivos
ls test_*.py

# Verificar sintaxis de tests
python -m pytest --collect-only
```

#### "ImportError" en mÃ³dulos del proyecto
El archivo `conftest.py` maneja automÃ¡ticamente los imports. Si persiste:
```bash
# Verificar que estÃ¡s en la carpeta correcta
pwd
# Debe mostrar: .../nlp-team2/server/tests
```

### Incompatibilidades Conocidas
- **Python 3.14 + FastAPI**: Error de pydantic que impide importaciÃ³n directa
- **SoluciÃ³n**: Los tests usan mocks y verificaciones estructurales

### Debug de Tests
```bash
# Ejecutar con debug
python -m pytest --pdb

# Ver output completo
python -m pytest -s -v

# Ejecutar test especÃ­fico con debug
python -m pytest test_main.py::test_specific_function -s -v
```

### Limpiar Cache
```bash
# Limpiar cache de pytest
rm -rf .pytest_cache/

# Limpiar cache de Python
find . -name "__pycache__" -exec rm -rf {} +

# Limpiar reportes anteriores
rm -rf htmlcov/
```

## ğŸ“ ConfiguraciÃ³n de Fixtures

### Fixtures Disponibles en conftest.py
- **Base de Datos**: `mock_db_session`, `mock_db_engine`, `sample_database_config`
- **Logging**: `sample_log_message`, `sample_log_levels`
- **Scraper**: `sample_youtube_urls`, `mock_selenium_element`, `mock_webdriver`
- **Utilidades**: `current_timestamp`, `sample_user_agents`, `project_paths`

### Estructura de Test Recomendada
```python
def test_function_name():
    # Arrange - Configurar datos de prueba
    setup_data = "test"
    
    # Act - Ejecutar la funciÃ³n a probar
    result = function_to_test(setup_data)
    
    # Assert - Verificar resultado
    assert result == expected_value
```

### Convenciones de Nomenclatura
- Archivos: `test_[mÃ³dulo].py`
- Funciones: `test_[funciÃ³n_especÃ­fica]()`
- Clases: `TestClassName`

## âš¡ Comandos RÃ¡pidos

```bash
# Ejecutar todos los tests con cobertura
./run_coverage.sh

# Tests rÃ¡pidos sin cobertura
python -m pytest

# Ver solo tests fallidos
python -m pytest --lf

# Ejecutar tests especÃ­ficos por palabra clave
python -m pytest -k "database"

# Modo verboso con detalles
python -m pytest -v

# Con salida en tiempo real
python -m pytest -s
```

## ğŸ¯ MÃ©tricas de Calidad

### Objetivos de Cobertura
- **MÃ³dulos core**: >90% (âœ… core/print_dev.py: 96%, core/config.py: 100%)
- **MÃ³dulos principales**: >70% (âš ï¸ pendiente)
- **Total del proyecto**: >80% (ğŸ”„ actual: 59%)

### Estado de Estabilidad
- **Tasa de Ã©xito**: 100% (94/94 tests)
- **Tests confiables**: âœ… Implementados
- **Mocks centralizados**: âœ… Configurados
- **CI/CD ready**: âœ… Scripts preparados
- **Archivos optimizados**: âœ… UnificaciÃ³n completada
- **Cobertura total**: 59% (2092 lÃ­neas)

---

**Para mÃ¡s informaciÃ³n sobre pytest**: [DocumentaciÃ³n oficial](https://docs.pytest.org/)  
**Para mÃ¡s informaciÃ³n sobre coverage**: [DocumentaciÃ³n oficial](https://coverage.readthedocs.io/)
